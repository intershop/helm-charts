apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "icm-as.fullname" . }}
  {{- if .Values.deploymentAnnotations }}
  annotations:
    {{- toYaml .Values.deploymentAnnotations | trim | nindent 4 }}
  {{- end }}
  labels:
    app: {{ include "icm-as.fullname" . }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}"
    {{- if .Values.datadog.enabled }}
    tags.datadoghq.com/env: {{ .Values.datadog.env }}
    tags.datadoghq.com/service: {{ .Values.datadog.service | default "icm-as" }}
    tags.datadoghq.com/version: {{ .Values.datadog.version | default .Values.image.tag | default .Chart.AppVersion }}
    {{- end }}
    {{- if .Values.deploymentLabels }}
      {{- toYaml .Values.deploymentLabels | trim | nindent 4 }}
    {{- end }}
spec:
  progressDeadlineSeconds: 600
  replicas: {{ .Values.replicaCount }}
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: {{ include "icm-as.fullname" . }}
      release: {{ .Release.Name }}
  strategy:
    type: Recreate
  template:
    metadata:
      {{- if .Values.podAnnotations }}
      annotations:
      {{- toYaml .Values.podAnnotations | trim | nindent 8 }}
      {{- end }}
      {{- if .Values.podBinding.enabled }}
      aadpodidbinding: {{ .Values.podBinding.binding }}
      {{- end }}
      labels:
        app: {{ include "icm-as.fullname" . }}
        release: {{ .Release.Name }}
        {{- if .Values.datadog.enabled }}
        tags.datadoghq.com/env: {{ .Values.datadog.env }}
        tags.datadoghq.com/service: {{ .Values.datadog.service | default "icm-as" }}
        tags.datadoghq.com/version: {{ .Values.datadog.version | default .Values.image.tag | default .Chart.AppVersion }}
        {{- end }}
        {{- if .Values.podLabels }}
          {{- toYaml .Values.podLabels | trim | nindent 8 }}
        {{- end }}
    spec:
      {{- if .Values.nodeSelector}}
      nodeSelector:
        {{- toYaml .Values.nodeSelector | trim | nindent 8 }}
      {{- end }}
      {{- if .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- range .Values.imagePullSecrets }}
        - name: {{ . | quote }}
        {{- end }}
      {{- end }}
      containers:
        - name: icm-as-server
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: "{{ .Values.image.pullPolicy }}"
          {{- if .Values.customCommand }}
          command:
            {{- toYaml .Values.customCommand | trim | nindent 10 }}
          {{- end }}
          env:
          - name: IS_DBPREPARE
            value: "false"
          - name: SERVER_NAME
            value: "appserver"
          {{- if .Values.jvm.debug.enabled }}
          - name: DEBUG_ICM
            value: "true"
          {{- end }}
          {{- if .Values.datadog.enabled }}
          - name: ENABLE_DATADOG
            value: "true"
          - name: DD_ENV
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tags.datadoghq.com/env']
          - name: DD_SERVICE
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tags.datadoghq.com/service']
          - name: DD_VERSION
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tags.datadoghq.com/version']
          - name: DD_LOGS_INJECTION
            value: "true"
          - name: DD_AGENT_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          {{- end }}
          {{- include "icm-as.featuredJVMArguments" . | nindent 10 }}
          {{- include "icm-as.additionalJVMArguments" . | nindent 10 }}
          {{- range $key, $value := .Values.environment }}
          - name: {{ $key }}
            value: {{ $value | quote }}
          {{- end }}
          ports:
            # Servlet engine connector port
            - name: http
              containerPort: 7743
              protocol: TCP
          {{- if .Values.jvm.debug.enabled }}
            # Java Debug port
            - name: debug
              containerPort: 7746
              protocol: TCP
          {{- end }}
            # JMX port
            - name: jmx
              containerPort: 7747
              protocol: TCP
          resources:
            {{- toYaml .Values.resources | trim | nindent 12 }}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          {{- if .Values.provideCustomConfig }}
          {{- range $k, $v := .Values.provideCustomConfig }}
            - mountPath: {{ $v.mountPath }}{{ $v.fileName }}
              name: {{ $k }}-volume
              subPath: {{ $v.fileName }}
          {{- end }}
          {{- end }}
            - mountPath: /intershop/license/license.xml
              name: license-volume
              readOnly: true
              {{- if eq .Values.license.type "configMap" }}
              subPath: license.xml
              {{- else if eq .Values.license.type "csi" }}
              subPath: license
              {{- end }}
            - mountPath: /intershop/sites
              name: sites-volume
          {{- if .Values.persistence.customdata.enabled }}
            - mountPath: {{ .Values.persistence.customdata.mountPoint }}
              name: custom-data-volume
          {{- end }}
            - mountPath: /intershop/customizations
              name: customizations-volume
            - mountPath: /intershop/jgroups-share
              name: jgroups-volume
          {{- if hasKey .Values.environment "STAGING_SYSTEM_TYPE" }}
          {{- if eq .Values.environment.STAGING_SYSTEM_TYPE "editing" }}
            - mountPath: /intershop/replication-conf/replication-clusters.xml
              name: replication-volume
              readOnly: true
              subPath: replication-clusters.xml
          {{- end }}
          {{- end }}
          startupProbe:
            httpGet:
              path: /status/LivenessProbe
              port: http
            # wait 60s then poll every 10s up to a total timeout of 120s
            failureThreshold: {{ .Values.probes.startup.failureThreshold | default 6 }}
            initialDelaySeconds: {{ .Values.probes.startup.initialDelaySeconds | default 60 }}
            periodSeconds: {{ .Values.probes.startup.periodSeconds | default 10 }}
          livenessProbe:
            httpGet:
              path: /status/LivenessProbe
              port: http
            #after startup: poll every 10s up to a total timeout of 30s
            failureThreshold: {{ .Values.probes.liveness.failureThreshold | default 3 }}
            initialDelaySeconds: {{ .Values.probes.liveness.initialDelaySeconds | default 0 }}
            periodSeconds: {{ .Values.probes.liveness.periodSeconds | default 10 }}
          readinessProbe:
            httpGet:
              path: /status/ReadinessProbe
              port: http
            #wait 60s (startup min) then poll every 5s up to a total timeout of 15s
            failureThreshold: {{ .Values.probes.readiness.failureThreshold | default 3 }}
            initialDelaySeconds: {{ .Values.probes.readiness.initialDelaySeconds | default 60 }}
            periodSeconds: {{ .Values.probes.readiness.periodSeconds | default 5 }}
      initContainers:
      {{- if eq .Values.persistence.sites.type "local" }}
        # the following container
        # 1) only is active if local storage is enabled
        # 2) applies permission 777 to sites volume
        # 3) makes user/group intershop owner of sites volume
        # !) This is necessary for Windows users with Docker Desktop using WSL[2] backend because:
        #    Docker Desktop with WSL[2] creates folders for local volume mounts assigning the user root and permissions 700
        - name: sites-volume-mount-hack
          image: busybox
          command: ["sh", "-c", "chmod 777 /intershop/sites && chown -R 150:150 /intershop/sites"]
          volumeMounts:
          - name: sites-volume
            mountPath: /intershop/sites
          securityContext:
            runAsUser: 0
      {{- end }}
      {{- if .Values.customizations }}
      {{- range $k, $v := .Values.customizations }}
        - name: {{ $k }}
          image: {{ $v.repository }}
          imagePullPolicy: {{ $v.pullPolicy | default "IfNotPresent" }}
          volumeMounts:
            - mountPath: /customizations
              name: customizations-volume
      {{- end }}
      {{- end }}
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      terminationGracePeriodSeconds: 30
      volumes:
      {{- if .Values.provideCustomConfig }}
      {{- range $k, $v := .Values.provideCustomConfig }}
      - name: {{ $k }}-volume
        configMap:
          name: {{ template "icm-as.fullname" $ }}-system-conf-cluster
          defaultMode: 420
      {{- end }}
      {{- end }}
      - name: license-volume
        {{- if eq .Values.license.type "configMap" }}
        configMap:
          defaultMode: 420
          name: {{ template "icm-as.fullname" . }}-license
        {{- else if eq .Values.license.type "csi" }}
        csi:
          driver: secrets-store.csi.k8s.io
          readOnly: true
{{ toYaml .Values.license.csi | indent 10 }}
        {{- end }}
      - name: jgroups-volume
        {{- if eq .Values.persistence.jgroups.type "local" }}
        persistentVolumeClaim:
          claimName: "{{ template "icm-as.fullname" . }}-local-jgroups-pvc"
        {{- else if eq .Values.persistence.jgroups.type "emptyDir" }}
        emptyDir: {}
        {{- else if eq .Values.persistence.jgroups.type "cluster" }}
        persistentVolumeClaim:
          claimName: "{{ template "icm-as.fullname" . }}-cluster-jgroups-pvc"
        {{- else if eq .Values.persistence.jgroups.type "existingClaim" }}
        persistentVolumeClaim:
          claimName: "{{ .Values.persistence.jgroups.existingClaim }}"
        {{- end }}
      - name: sites-volume
        {{- if eq .Values.persistence.sites.type "local" }}
        persistentVolumeClaim:
          claimName: "{{ template "icm-as.fullname" . }}-local-sites-pvc"
        {{- else if eq .Values.persistence.sites.type "existingClaim" }}
        persistentVolumeClaim:
          claimName: "{{ .Values.persistence.sites.existingClaim }}"
        {{- else if eq .Values.persistence.sites.type "azurefiles" }}
        azureFile:
          secretName: {{ .Values.persistence.sites.azurefiles.secretName }}
          shareName: {{ .Values.persistence.sites.azurefiles.shareName }}
          readOnly: false
        {{- else if eq .Values.persistence.sites.type "nfs" }}
        persistentVolumeClaim:
          claimName: "{{ template "icm-as.fullname" . }}-nfs-sites-pvc"
        {{- else if eq .Values.persistence.sites.type "cluster" }}
        persistentVolumeClaim:
          claimName: "{{ template "icm-as.fullname" . }}-cluster-sites-pvc"
        {{- end }}
        {{- if hasKey .Values.environment "STAGING_SYSTEM_TYPE" }}
        {{- if eq .Values.environment.STAGING_SYSTEM_TYPE "editing" }}
      - name: replication-volume
        configMap:
          name: {{ template "icm-as.fullname" . }}-replication-clusters-xml
        {{- end }}
        {{- end }}
        {{- if .Values.persistence.customdata.enabled }}
      - name: custom-data-volume
        persistentVolumeClaim:
          claimName: "{{ .Values.persistence.customdata.existingClaim }}"
        {{- end }}
      - name: customizations-volume
        emptyDir: {}
